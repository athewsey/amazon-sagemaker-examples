{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1066c429",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using PyTorch\n",
    "\n",
    "> *This notebook works well with SageMaker Studio kernel `Python 3 (Data Science)`, or SageMaker Notebook Instance kernel `conda_python3`*\n",
    "\n",
    "With [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.pytorch import PyTorch as PyTorchEstimator, PyTorchModel\n",
    "\n",
    "smsess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Configuration:\n",
    "bucket_name = smsess.default_bucket()\n",
    "prefix = \"mnist/\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix[:-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d8b79",
   "metadata": {},
   "source": [
    "## The example use case: MNIST\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images.\n",
    "\n",
    "In this example, we download the MNIST data from a public S3 bucket and upload it to your default SageMaker bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sample_data(to_bucket, to_prefix, from_bucket=\"sagemaker-sample-files\", from_prefix=\"datasets/image/MNIST\", dataset=\"mnist-train\"):\n",
    "    DATASETS = {\n",
    "        \"mnist-train\": [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\"],\n",
    "        \"mnist-test\": [\"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"],\n",
    "    }\n",
    "\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"dataset '{dataset}' not in known set: {set(DATASETS.keys())}\")\n",
    "\n",
    "    if len(from_prefix) and not from_prefix.endswith(\"/\"):\n",
    "        from_prefix += \"/\"\n",
    "    if len(to_prefix) and not to_prefix.endswith(\"/\"):\n",
    "        to_prefix += \"/\"\n",
    "\n",
    "    s3client = boto3.client(\"s3\")\n",
    "    for key in DATASETS[dataset]:\n",
    "        s3client.copy_object(\n",
    "            CopySource={\n",
    "                \"Bucket\": from_bucket,\n",
    "                \"Key\": f\"{from_prefix}{key}\",\n",
    "            },\n",
    "            Bucket=to_bucket,\n",
    "            Key=f\"{to_prefix}{key}\",\n",
    "        )\n",
    "\n",
    "train_prefix = f\"{prefix}data/train\"\n",
    "fetch_sample_data(to_bucket=bucket_name, to_prefix=train_prefix, dataset=\"mnist-train\")\n",
    "train_s3uri = f\"s3://{bucket_name}/{train_prefix}\"\n",
    "print(f\"Uploaded training data to {train_s3uri}\")\n",
    "\n",
    "test_prefix = f\"{prefix}data/test\"\n",
    "fetch_sample_data(to_bucket=bucket_name, to_prefix=test_prefix, dataset=\"mnist-test\")\n",
    "test_s3uri = f\"s3://{bucket_name}/{test_prefix}\"\n",
    "print(f\"Uploaded training data to {test_s3uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data:\")\n",
    "!aws s3 ls --recursive $train_s3uri\n",
    "print(\"Test data:\")\n",
    "!aws s3 ls --recursive $test_s3uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35023d",
   "metadata": {},
   "source": [
    "## Train multiple models\n",
    "\n",
    "In the following section, we'll train multiple models on the same dataset, using the SageMaker PyTorch Framework Container.\n",
    "\n",
    "To keep things simple, we'll create two models `A` and `B`, using the same code but some slightly different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3012fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(base_job_name, hyperparam_overrides={}):\n",
    "    hyperparameters = {\n",
    "        'batch-size':128,\n",
    "        'epochs':20,\n",
    "        'learning-rate': 1e-3,\n",
    "        'log-interval':100,\n",
    "    }\n",
    "    for k, v in hyperparam_overrides.items():\n",
    "        hyperparameters[k] = v\n",
    "\n",
    "    return PyTorchEstimator(\n",
    "        base_job_name=base_job_name,\n",
    "        entry_point='train.py',\n",
    "        source_dir='code', # directory of your training script\n",
    "        role=role,\n",
    "        framework_version='1.8',\n",
    "        py_version='py3',\n",
    "        instance_type=\"ml.c4.xlarge\",\n",
    "        instance_count=1,\n",
    "        output_path=output_path,\n",
    "        hyperparameters=hyperparameters,\n",
    "    )\n",
    "\n",
    "estimatorA = get_estimator(base_job_name=\"mnist-a\", hyperparam_overrides={ \"weight-decay\": 1e-4 })\n",
    "estimatorB = get_estimator(base_job_name=\"mnist-b\", hyperparam_overrides={ \"weight-decay\": 1e-2 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba26ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimatorA.fit({ \"training\": train_s3uri, \"testing\": test_s3uri }, wait=False)\n",
    "print(\"Started estimator A training in background (logs will not show)\")\n",
    "\n",
    "print(\"Training estimator B with logs:\")\n",
    "estimatorB.fit({ \"training\": train_s3uri, \"testing\": test_s3uri })\n",
    "\n",
    "print(\"\\nWaiting for estimator A to complete:\")\n",
    "estimatorA.latest_training_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec71606",
   "metadata": {},
   "source": [
    "## Check single-model deployment\n",
    "\n",
    "Before we try to set up a multi-model deployment, let's quickly check we're able to deploy and call a single model as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ff402",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = estimatorA.create_model(name=\"mnist-a\", role=role, source_dir=\"code\", entry_point=\"inference.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorA = modelA.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    ")\n",
    "predictorA.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictorA.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e37992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_request():\n",
    "    \"\"\"Create a dummy predictor.predict example data (16 images of random pixels)\"\"\"\n",
    "    return {\n",
    "        \"inputs\": np.random.rand(16, 1, 28, 28).tolist()\n",
    "    }\n",
    "\n",
    "dummy_data = get_dummy_request()\n",
    "\n",
    "start_time = time.time()\n",
    "predicted_value = predictorA.predict(dummy_data)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"Model took {int(duration * 1000):,d} ms\")\n",
    "np.array(predicted_value)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a189a0e",
   "metadata": {},
   "source": [
    "## Create the multi-model endpoint with the SageMaker SDK\n",
    "\n",
    "### Create a SageMaker Model from one of the Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee485a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = estimatorA.create_model(role=role, source_dir=\"code\", entry_point=\"inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84f3ce",
   "metadata": {},
   "source": [
    "### Create the Amazon SageMaker MultiDataModel entity\n",
    "\n",
    "We create the multi-model endpoint using the [```MultiDataModel```](https://sagemaker.readthedocs.io/en/stable/api/inference/multi_data_model.html) class.\n",
    "\n",
    "You can create a MultiDataModel by directly passing in a `sagemaker.model.Model` object - in which case, the Endpoint will inherit information about the image to use, as well as any environmental variables, network isolation, etc., once the MultiDataModel is deployed.\n",
    "\n",
    "In addition, a MultiDataModel can also be created without explictly passing a `sagemaker.model.Model` object. Please refer to the documentation for additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57034adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where our MME will read models from on S3.\n",
    "multi_model_prefix = f\"{prefix}multi-model/\"\n",
    "multi_model_s3uri = f\"s3://{bucket_name}/{multi_model_prefix}\"\n",
    "print(multi_model_s3uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f730c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mme = MultiDataModel(\n",
    "    name=\"mnist-multi\",\n",
    "    model_data_prefix=multi_model_s3uri,\n",
    "    model=model,  # passing our model\n",
    "    sagemaker_session=smsess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e08f6",
   "metadata": {},
   "source": [
    "## Deploy the Multi Model Endpoint\n",
    "\n",
    "You need to consider the appropriate instance type and number of instances for the projected prediction workload across all the models you plan to host behind your multi-model endpoint. The number and size of the individual models will also drive memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a371849",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint(delete_endpoint_configuration=True)\n",
    "    print(\"Deleting previous endpoint...\")\n",
    "    time.sleep(10)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "predictor = mme.deploy(\n",
    "    endpoint_name=\"mnist-multi\",\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d60b69",
   "metadata": {},
   "source": [
    "### Our endpoint has launched! Let's look at what models are available to the endpoint!\n",
    "\n",
    "By 'available', what we mean is, what model artfiacts are currently stored under the S3 prefix we defined when setting up the `MultiDataModel` above i.e. `model_data_prefix`.\n",
    "\n",
    "Currently, since we have no artifacts (i.e. `tar.gz` files) stored under  our defined S3 prefix, our endpoint, will have no models 'available' to serve inference requests.\n",
    "\n",
    "We will demonstrate how to make models 'available' to our endpoint below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22db735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No models visible!\n",
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2360af",
   "metadata": {},
   "source": [
    "### Lets deploy model artifacts to be found by the endpoint\n",
    "\n",
    "We are now using the `.add_model()` method of the `MultiDataModel` to copy over our model artifacts from where they were initially stored, during training, to where our endpoint will source model artifacts for inference requests.\n",
    "\n",
    "`model_data_source` refers to the location of our model artifact (i.e. where it was deposited on S3 after training completed)\n",
    "\n",
    "`model_data_path` is the **relative** path to the S3 prefix we specified above (i.e. `model_data_prefix`) where our endpoint will source models for inference requests.\n",
    "\n",
    "Since this is a **relative** path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. `Chicago_IL.tar.gz`)\n",
    "\n",
    "### Dynamically deploying additional models\n",
    "\n",
    "It is also important to note, that we can always use the `.add_model()` method, as shown below, to dynamically deploy more models to the endpoint, to serve up inference requests as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, est in { \"ModelA\": estimatorA, \"ModelB\": estimatorB }.items():\n",
    "    artifact_path = est.latest_training_job.describe()[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "    #model_name = artifact_path.split('/')[-4]+'.tar.gz'\n",
    "    # This is copying over the model artifact to the S3 location for the MME.\n",
    "    mme.add_model(model_data_source=artifact_path, model_data_path=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4fd1a0",
   "metadata": {},
   "source": [
    "## We have added the 4 model artifacts from our training jobs!\n",
    "\n",
    "We can see that the S3 prefix we specified when setting up `MultiDataModel` now has 4 model artifacts. As such, the endpoint can now serve up inference requests for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecfe46",
   "metadata": {},
   "source": [
    "# Get predictions from the endpoint\n",
    "\n",
    "Recall that ```mme.deploy()``` returns a [RealTimePredictor](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/predictor.py#L35) that we saved in a variable called ```predictor```.\n",
    "\n",
    "We will use ```predictor``` to submit requests to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = get_dummy_request()\n",
    "\n",
    "start_time = time.time()\n",
    "predicted_value = predictor.predict(dummy_data, target_model=\"ModelA\")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"Model took {int(duration * 1000):,d} ms\")\n",
    "np.array(predicted_value)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18bcc33",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86db770",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorA.delete_endpoint(delete_endpoint_config=True)\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ed4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
